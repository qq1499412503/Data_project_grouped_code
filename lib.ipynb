{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train data:  (50000, 32, 32, 3)\nTrain filenames:  (50000,)\nTrain labels:  (50000,)\nTest data:  (10000, 32, 32, 3)\nTest filenames:  (10000,)\nTest labels:  (10000,)\nLabel names:  (10,)\n[b'leptodactylus_pentadactylus_s_000004.png' b'camion_s_000148.png'\n b'tipper_truck_s_001250.png' ... b'tipper_s_000416.png'\n b'coupe_s_001887.png' b'coupe_s_001616.png']\n[6 9 9 ... 9 1 1]\n",
      "[[[[0.61960787 0.4392157  0.19215687]\n   [0.62352943 0.43529412 0.18431373]\n   [0.64705884 0.45490196 0.2       ]\n   ...\n   [0.5372549  0.37254903 0.14117648]\n   [0.49411765 0.35686275 0.14117648]\n   [0.45490196 0.33333334 0.12941177]]\n\n  [[0.59607846 0.4392157  0.2       ]\n   [0.5921569  0.43137255 0.15686275]\n   [0.62352943 0.44705883 0.1764706 ]\n   ...\n   [0.53333336 0.37254903 0.12156863]\n   [0.49019608 0.35686275 0.1254902 ]\n   [0.46666667 0.34509805 0.13333334]]\n\n  [[0.5921569  0.43137255 0.18431373]\n   [0.5921569  0.42745098 0.12941177]\n   [0.61960787 0.43529412 0.14117648]\n   ...\n   [0.54509807 0.38431373 0.13333334]\n   [0.50980395 0.37254903 0.13333334]\n   [0.47058824 0.34901962 0.12941177]]\n\n  ...\n\n  [[0.26666668 0.4862745  0.69411767]\n   [0.16470589 0.39215687 0.5803922 ]\n   [0.12156863 0.34509805 0.5372549 ]\n   ...\n   [0.14901961 0.38039216 0.57254905]\n   [0.05098039 0.2509804  0.42352942]\n   [0.15686275 0.33333334 0.49803922]]\n\n  [[0.23921569 0.45490196 0.65882355]\n   [0.19215687 0.4        0.5803922 ]\n   [0.13725491 0.33333334 0.5176471 ]\n   ...\n   [0.10196079 0.32156864 0.50980395]\n   [0.11372549 0.32156864 0.49411765]\n   [0.07843138 0.2509804  0.41960785]]\n\n  [[0.21176471 0.41960785 0.627451  ]\n   [0.21960784 0.4117647  0.58431375]\n   [0.1764706  0.34901962 0.5176471 ]\n   ...\n   [0.09411765 0.3019608  0.4862745 ]\n   [0.13333334 0.32941177 0.5058824 ]\n   [0.08235294 0.2627451  0.43137255]]]\n\n\n [[[0.92156863 0.92156863 0.92156863]\n   [0.90588236 0.90588236 0.90588236]\n   [0.9098039  0.9098039  0.9098039 ]\n   ...\n   [0.9137255  0.9137255  0.9137255 ]\n   [0.9137255  0.9137255  0.9137255 ]\n   [0.9098039  0.9098039  0.9098039 ]]\n\n  [[0.93333334 0.93333334 0.93333334]\n   [0.92156863 0.92156863 0.92156863]\n   [0.92156863 0.92156863 0.92156863]\n   ...\n   [0.9254902  0.9254902  0.9254902 ]\n   [0.9254902  0.9254902  0.9254902 ]\n   [0.92156863 0.92156863 0.92156863]]\n\n  [[0.92941177 0.92941177 0.92941177]\n   [0.91764706 0.91764706 0.91764706]\n   [0.91764706 0.91764706 0.91764706]\n   ...\n   [0.92156863 0.92156863 0.92156863]\n   [0.92156863 0.92156863 0.92156863]\n   [0.91764706 0.91764706 0.91764706]]\n\n  ...\n\n  [[0.34117648 0.3882353  0.34901962]\n   [0.16862746 0.2        0.14509805]\n   [0.07450981 0.09019608 0.04313726]\n   ...\n   [0.6627451  0.72156864 0.7019608 ]\n   [0.7137255  0.77254903 0.75686276]\n   [0.7372549  0.7921569  0.7882353 ]]\n\n  [[0.32156864 0.3764706  0.32156864]\n   [0.18039216 0.22352941 0.14117648]\n   [0.14117648 0.17254902 0.08627451]\n   ...\n   [0.68235296 0.7411765  0.7176471 ]\n   [0.7254902  0.78431374 0.76862746]\n   [0.73333335 0.7921569  0.78431374]]\n\n  [[0.33333334 0.39607844 0.3254902 ]\n   [0.24313726 0.29411766 0.1882353 ]\n   [0.22745098 0.2627451  0.14901961]\n   ...\n   [0.65882355 0.7176471  0.69803923]\n   [0.7058824  0.7647059  0.7490196 ]\n   [0.7294118  0.78431374 0.78039217]]]\n\n\n [[[0.61960787 0.74509805 0.87058824]\n   [0.61960787 0.73333335 0.85490197]\n   [0.54509807 0.6509804  0.7607843 ]\n   ...\n   [0.89411765 0.90588236 0.91764706]\n   [0.92941177 0.9372549  0.9529412 ]\n   [0.93333334 0.94509804 0.9647059 ]]\n\n  [[0.6666667  0.78431374 0.8980392 ]\n   [0.6745098  0.78039217 0.8862745 ]\n   [0.5921569  0.6901961  0.7882353 ]\n   ...\n   [0.9098039  0.9098039  0.9254902 ]\n   [0.9647059  0.9647059  0.98039216]\n   [0.9647059  0.96862745 0.9843137 ]]\n\n  [[0.68235296 0.7882353  0.88235295]\n   [0.6901961  0.78431374 0.87058824]\n   [0.6156863  0.7019608  0.78039217]\n   ...\n   [0.9019608  0.8980392  0.9098039 ]\n   [0.98039216 0.9764706  0.9843137 ]\n   [0.9607843  0.95686275 0.96862745]]\n\n  ...\n\n  [[0.12156863 0.15686275 0.1764706 ]\n   [0.11764706 0.15294118 0.17254902]\n   [0.10196079 0.13725491 0.15686275]\n   ...\n   [0.14509805 0.15686275 0.18039216]\n   [0.03529412 0.05098039 0.05490196]\n   [0.01568628 0.02745098 0.01960784]]\n\n  [[0.09019608 0.13333334 0.15294118]\n   [0.10588235 0.14901961 0.16862746]\n   [0.09803922 0.14117648 0.16078432]\n   ...\n   [0.07450981 0.07843138 0.09411765]\n   [0.01568628 0.02352941 0.01176471]\n   [0.01960784 0.02745098 0.01176471]]\n\n  [[0.10980392 0.16078432 0.18431373]\n   [0.11764706 0.16862746 0.19607843]\n   [0.1254902  0.1764706  0.20392157]\n   ...\n   [0.01960784 0.02352941 0.03137255]\n   [0.01568628 0.01960784 0.01176471]\n   [0.02745098 0.03137255 0.02745098]]]\n\n\n ...\n\n\n [[[0.07843138 0.05882353 0.04705882]\n   [0.07450981 0.05490196 0.04313726]\n   [0.05882353 0.05490196 0.04313726]\n   ...\n   [0.03921569 0.03529412 0.02745098]\n   [0.04705882 0.04313726 0.03529412]\n   [0.05098039 0.04705882 0.03921569]]\n\n  [[0.08235294 0.0627451  0.05098039]\n   [0.07843138 0.0627451  0.05098039]\n   [0.07058824 0.06666667 0.04705882]\n   ...\n   [0.03921569 0.03529412 0.02745098]\n   [0.03921569 0.03529412 0.02745098]\n   [0.04705882 0.04313726 0.03529412]]\n\n  [[0.08235294 0.0627451  0.05098039]\n   [0.08235294 0.06666667 0.04705882]\n   [0.07843138 0.07058824 0.04313726]\n   ...\n   [0.04705882 0.04313726 0.03529412]\n   [0.04705882 0.04313726 0.03529412]\n   [0.05098039 0.04705882 0.03921569]]\n\n  ...\n\n  [[0.12941177 0.09803922 0.05098039]\n   [0.13333334 0.10196079 0.05882353]\n   [0.13333334 0.10196079 0.05882353]\n   ...\n   [0.10980392 0.09803922 0.20392157]\n   [0.11372549 0.09803922 0.22745098]\n   [0.09019608 0.07843138 0.16470589]]\n\n  [[0.12941177 0.09803922 0.05490196]\n   [0.13333334 0.10196079 0.05882353]\n   [0.13333334 0.10196079 0.05882353]\n   ...\n   [0.10588235 0.09411765 0.20392157]\n   [0.10588235 0.09411765 0.21960784]\n   [0.09803922 0.08627451 0.18431373]]\n\n  [[0.12156863 0.09019608 0.04705882]\n   [0.1254902  0.09411765 0.05098039]\n   [0.12941177 0.09803922 0.05490196]\n   ...\n   [0.09411765 0.09019608 0.19607843]\n   [0.10196079 0.09019608 0.20784314]\n   [0.09803922 0.07843138 0.18431373]]]\n\n\n [[[0.09803922 0.15686275 0.04705882]\n   [0.05882353 0.14117648 0.01176471]\n   [0.09019608 0.16078432 0.07058824]\n   ...\n   [0.23921569 0.32156864 0.30588236]\n   [0.36078432 0.44313726 0.4392157 ]\n   [0.29411766 0.34901962 0.36078432]]\n\n  [[0.04705882 0.09803922 0.02352941]\n   [0.07843138 0.14509805 0.02745098]\n   [0.09411765 0.14117648 0.05882353]\n   ...\n   [0.4509804  0.5254902  0.5411765 ]\n   [0.58431375 0.65882355 0.69411767]\n   [0.40784314 0.45882353 0.5137255 ]]\n\n  [[0.04705882 0.09803922 0.04313726]\n   [0.05882353 0.11372549 0.02352941]\n   [0.13333334 0.15686275 0.09411765]\n   ...\n   [0.6039216  0.6745098  0.7137255 ]\n   [0.6156863  0.6862745  0.7529412 ]\n   [0.45490196 0.5058824  0.5921569 ]]\n\n  ...\n\n  [[0.39215687 0.5058824  0.31764707]\n   [0.40392157 0.5176471  0.32941177]\n   [0.40784314 0.5254902  0.3372549 ]\n   ...\n   [0.38039216 0.5019608  0.32941177]\n   [0.38431373 0.49411765 0.32941177]\n   [0.35686275 0.4745098  0.30980393]]\n\n  [[0.40392157 0.5176471  0.3254902 ]\n   [0.40784314 0.5137255  0.3254902 ]\n   [0.41960785 0.5294118  0.34117648]\n   ...\n   [0.39607844 0.5176471  0.34117648]\n   [0.3882353  0.49803922 0.32941177]\n   [0.36078432 0.4745098  0.30980393]]\n\n  [[0.37254903 0.49411765 0.30588236]\n   [0.37254903 0.48235294 0.29803923]\n   [0.39607844 0.5019608  0.31764707]\n   ...\n   [0.3647059  0.4862745  0.3137255 ]\n   [0.37254903 0.48235294 0.31764707]\n   [0.36078432 0.47058824 0.3137255 ]]]\n\n\n [[[0.28627452 0.30588236 0.29411766]\n   [0.38431373 0.40392157 0.44313726]\n   [0.3882353  0.41568628 0.44705883]\n   ...\n   [0.5294118  0.5882353  0.59607846]\n   [0.5294118  0.58431375 0.6039216 ]\n   [0.79607844 0.84313726 0.8745098 ]]\n\n  [[0.27058825 0.28627452 0.27450982]\n   [0.32941177 0.34901962 0.38039216]\n   [0.26666668 0.29411766 0.31764707]\n   ...\n   [0.33333334 0.37254903 0.34901962]\n   [0.2784314  0.32156864 0.3137255 ]\n   [0.47058824 0.52156866 0.5294118 ]]\n\n  [[0.27058825 0.28627452 0.27450982]\n   [0.3529412  0.37254903 0.39215687]\n   [0.24313726 0.2784314  0.2901961 ]\n   ...\n   [0.2901961  0.31764707 0.27450982]\n   [0.20784314 0.24313726 0.21176471]\n   [0.24313726 0.2901961  0.27058825]]\n\n  ...\n\n  [[0.48235294 0.5019608  0.3764706 ]\n   [0.5176471  0.5176471  0.4       ]\n   [0.5058824  0.5019608  0.39215687]\n   ...\n   [0.42352942 0.41960785 0.34509805]\n   [0.24313726 0.23529412 0.21568628]\n   [0.10588235 0.10588235 0.10980392]]\n\n  [[0.4509804  0.4745098  0.35686275]\n   [0.48235294 0.4862745  0.37254903]\n   [0.5058824  0.49411765 0.3882353 ]\n   ...\n   [0.4509804  0.45490196 0.36862746]\n   [0.25882354 0.25490198 0.23137255]\n   [0.10588235 0.10588235 0.10588235]]\n\n  [[0.45490196 0.47058824 0.3529412 ]\n   [0.4745098  0.47843137 0.36862746]\n   [0.5058824  0.5019608  0.39607844]\n   ...\n   [0.45490196 0.4509804  0.36862746]\n   [0.26666668 0.25490198 0.22745098]\n   [0.10588235 0.10196079 0.10196079]]]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#import cifar10\n",
    "\n",
    "\"\"\"\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 \n",
    "training images and 10000 test images.\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains \n",
    "exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random \n",
    "order, but some training batches may contain more images from one class than another. Between them, the training \n",
    "batches contain exactly 5000 images from each class.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    \"\"\"load the cifar-10 data\"\"\"\n",
    "\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cifar_10_data(data_dir, negatives=False):\n",
    "    \"\"\"\n",
    "    Return train_data, train_filenames, train_labels, test_data, test_filenames, test_labels\n",
    "    \"\"\"\n",
    "\n",
    "    # get the meta_data_dict\n",
    "    # num_cases_per_batch: 1000\n",
    "    # label_names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    # num_vis: :3072\n",
    "\n",
    "    meta_data_dict = unpickle(data_dir + \"/batches.meta\")\n",
    "    cifar_label_names = meta_data_dict[b'label_names']\n",
    "    cifar_label_names = np.array(cifar_label_names)\n",
    "\n",
    "    # training data\n",
    "    cifar_train_data = None\n",
    "    cifar_train_filenames = []\n",
    "    cifar_train_labels = []\n",
    "\n",
    "    # cifar_train_data_dict\n",
    "    # 'batch_label': 'training batch 5 of 5'\n",
    "    # 'data': ndarray\n",
    "    # 'filenames': list\n",
    "    # 'labels': list\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        cifar_train_data_dict = unpickle(data_dir + \"/data_batch_{}\".format(i))\n",
    "        if i == 1:\n",
    "            cifar_train_data = cifar_train_data_dict[b'data']\n",
    "        else:\n",
    "            cifar_train_data = np.vstack((cifar_train_data, cifar_train_data_dict[b'data']))\n",
    "        cifar_train_filenames += cifar_train_data_dict[b'filenames']\n",
    "        cifar_train_labels += cifar_train_data_dict[b'labels']\n",
    "\n",
    "    cifar_train_data = cifar_train_data.reshape((len(cifar_train_data), 3, 32, 32))\n",
    "    if negatives:\n",
    "        cifar_train_data = cifar_train_data.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    else:\n",
    "        cifar_train_data = np.rollaxis(cifar_train_data, 1, 4)\n",
    "    cifar_train_filenames = np.array(cifar_train_filenames)\n",
    "    cifar_train_labels = np.array(cifar_train_labels)\n",
    "\n",
    "    # test data\n",
    "    # cifar_test_data_dict\n",
    "    # 'batch_label': 'testing batch 1 of 1'\n",
    "    # 'data': ndarray\n",
    "    # 'filenames': list\n",
    "    # 'labels': list\n",
    "\n",
    "    cifar_test_data_dict = unpickle(data_dir + \"/test_batch\")\n",
    "    cifar_test_data = cifar_test_data_dict[b'data']\n",
    "    cifar_test_filenames = cifar_test_data_dict[b'filenames']\n",
    "    cifar_test_labels = cifar_test_data_dict[b'labels']\n",
    "\n",
    "    cifar_test_data = cifar_test_data.reshape((len(cifar_test_data), 3, 32, 32))\n",
    "    if negatives:\n",
    "        cifar_test_data = cifar_test_data.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    else:\n",
    "        cifar_test_data = np.rollaxis(cifar_test_data, 1, 4)\n",
    "    cifar_test_filenames = np.array(cifar_test_filenames)\n",
    "    cifar_test_labels = np.array(cifar_test_labels)\n",
    "\n",
    "    return cifar_train_data, cifar_train_filenames, cifar_train_labels, \\\n",
    "        cifar_test_data, cifar_test_filenames, cifar_test_labels, cifar_label_names\n",
    "\n",
    "\n",
    "cifar_10_dir = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = \\\n",
    "load_cifar_10_data(cifar_10_dir)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Train filenames: \", train_filenames.shape)\n",
    "print(\"Train labels: \", train_labels.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "print(\"Test filenames: \", test_filenames.shape)\n",
    "print(\"Test labels: \", test_labels.shape)\n",
    "print(\"Label names: \", label_names.shape)\n",
    "print(train_filenames)\n",
    "print(train_labels)  \n",
    "\n",
    "#convert to binary \n",
    "num_classes = 10\n",
    "def binary_label(label, num_class):\n",
    "    return keras.utils.to_categorical(label, num_class)\n",
    "\n",
    "train_labels = binary_label(train_labels, num_classes)\n",
    "test_labels = binary_label(test_labels, num_classes)\n",
    "\n",
    "def image_normalization(image):\n",
    "    return image.astype('float32')/255\n",
    "\n",
    "train_data = image_normalization(train_data)\n",
    "test_data = image_normalization(test_data)\n",
    "print(test_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}